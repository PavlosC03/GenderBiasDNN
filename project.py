# -*- coding: utf-8 -*-
"""Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16k7QkG-ew9ZkDeBOuyq2qhigiYFzYwp_

https://www.kaggle.com/datasets/amrmaree/student-performance-prediction?resource=download
"""

# Install the Rapid Deep Neural Networks library
# !pip install radnn -q
# !pip list | grep "radnn"

# from radnn.system.hosts import ColabHost

# Mount to the project folder that is under the lesson folder
# ColabHost().detect_workspace(["CS345", "MSDS680"]).change_to_project_dir("Project")

"""Importing required packages"""

import os
os.environ["TF_USE_LEGACY_KERAS"]="1"   # Use Keras 2.x. We set this before the first import of tensorflow
script_dir = os.path.dirname(os.path.abspath(__file__))
os.chdir(script_dir)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
import seaborn as sb
import json

from student_performance_dataset import StudentPerformanceDataset
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder

from radnn import FileSystem
from radnn.experiment import MLExperimentConfig
from radnn.evaluation import EvaluateClassification
from radnn.plots import PlotLearningCurve, PlotConfusionMatrix


# __________ | Settings | __________
IS_PLOTING_DATA         = True
IS_DEBUGABLE            = False
IS_RETRAINING           = True
RANDOM_SEED             = 2025

oFileSys = FileSystem("MLConfig", "MLModels", "MLData")

"""Hyperparameters (with gender)"""

oConfig = MLExperimentConfig(number=1).assign({
    "ModelName": "STUDENT_PERFORMANCE_DNN",
    "DNN.InputFeatures": 7,
    "DNN.LayerNeurons": [24, 32, 32, 1],
    "DNN.Classes": 1,
    "DNN.DropoutRate": 0.1,
    "Training.MaxEpoch": 300,
    "Training.BatchSize": 256,
    "Training.LearningRate": 0.01
}).save(oFileSys)

"""Hyperparameters (without gender)"""

oConfig = MLExperimentConfig(number=1).assign({
    "ModelName": "STUDENT_PERFORMANCE_DNN",
    "DNN.InputFeatures": 6,
    "DNN.LayerNeurons": [24, 32, 32, 1],
    "DNN.Classes": 1,
    "DNN.DropoutRate": 0.1,
    "Training.MaxEpoch": 300,
    "Training.BatchSize": 256,
    "Training.LearningRate": 0.01
}).save(oFileSys)

"""Data preprocessing"""

oDatasetFS = oFileSys.datasets.subfs("STUDENTS")
sFileName = oDatasetFS.file("student_performance_dataset.csv")
oDataSet = StudentPerformanceDataset(sFileName)
print(f"The dataset would be imported from\n {sFileName}\n and maintained under {oDatasetFS.base_folder}")

"""Verifying that the dataset can be read"""

oDF = pd.read_csv(sFileName, delimiter=",");
oDF.head()

"""Drop gender column"""

oDF_no_gender = oDF.drop(columns=['Gender'])

"""Data Preprocessing (with gender)"""

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# create the StudentPerformanceDataset subclass
class StudentPerformanceDataset:
    def __init__(self, file_path):
        self.file_path = file_path
        self.df = None
        self.X_train = None
        self.y_train = None
        self.X_val = None
        self.y_val = None
        self.scaler = MinMaxScaler()

    def load_and_process_data(self):
        self.df = pd.read_csv(self.file_path)

        # remove non-numeric columns
        self.df.drop(columns=['Student_ID'], inplace=True)

        categorical_columns = ['Gender', 'Parental_Education_Level', 'Internet_Access_at_Home',
                               'Extracurricular_Activities', 'Pass_Fail']

        # label encoding
        le = LabelEncoder()
        for col in categorical_columns:
            if col in self.df.columns:
                self.df[col] = le.fit_transform(self.df[col])

        # normalize numerical columns
        numerical_columns = ['Study_Hours_per_Week', 'Attendance_Rate', 'Past_Exam_Scores', 'Final_Exam_Score']
        self.df[numerical_columns] = self.scaler.fit_transform(self.df[numerical_columns])

        X = self.df.drop(columns=['Final_Exam_Score', 'Pass_Fail']).values
        y = self.df['Pass_Fail'].values

        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2025)

        self.X_train, self.X_val = X_train, X_val
        self.y_train, self.y_val = y_train, y_val

    def get_train_data(self):
        return self.X_train, self.y_train

    def get_val_data(self):
        return self.X_val, self.y_val


dataset = StudentPerformanceDataset("data/student_performance_dataset.csv")
dataset.load_and_process_data()
X_train, y_train = dataset.get_train_data()
X_val, y_val = dataset.get_val_data()

print("Training Data Shape:", X_train.shape)
print("Validation Data Shape:", X_val.shape)

"""Data preprocessing (without gender)"""

import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import numpy as np

# create the StudentPerformanceDataset subclass
class StudentPerformanceDataset:
    def __init__(self, file_path):
        self.file_path = file_path
        self.df = None
        self.X_train = None
        self.y_train = None
        self.X_val = None
        self.y_val = None
        self.scaler = MinMaxScaler()

    def load_and_process_data(self):
        self.df = pd.read_csv(self.file_path)

        # remove non-numeric and gender columns
        self.df.drop(columns=['Student_ID', 'Gender'], inplace=True)  # Drop 'Gender' here

        categorical_columns = ['Parental_Education_Level', 'Internet_Access_at_Home',
                               'Extracurricular_Activities', 'Pass_Fail']

        # label encoding
        le = LabelEncoder()
        for col in categorical_columns:
            if col in self.df.columns:
                self.df[col] = le.fit_transform(self.df[col])

        # normalize numerical columns
        numerical_columns = ['Study_Hours_per_Week', 'Attendance_Rate', 'Past_Exam_Scores', 'Final_Exam_Score']
        self.df[numerical_columns] = self.scaler.fit_transform(self.df[numerical_columns])

        X = self.df.drop(columns=['Final_Exam_Score', 'Pass_Fail']).values
        y = self.df['Pass_Fail'].values

        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2025)

        self.X_train, self.X_val = X_train, X_val
        self.y_train, self.y_val = y_train, y_val

    def get_train_data(self):
        return self.X_train, self.y_train

    def get_val_data(self):
        return self.X_val, self.y_val


dataset = StudentPerformanceDataset("data/student_performance_dataset.csv")
dataset.load_and_process_data()  # This should work now

X_train, y_train = dataset.get_train_data()
X_val, y_val = dataset.get_val_data()

print("Training Data Shape:", X_train.shape)
print("Validation Data Shape:", X_val.shape)

"""DNN"""

import keras
from keras.layers import Dense, Dropout, BatchNormalization
from keras.layers import Activation, Softmax

class CDNN(keras.Model):
  # --------------------------------------------------------------------------------------
  def __init__(self, config):
    super(CDNN, self).__init__()
    # ..................... Object Attributes ...........................
    self.Config = config
    self.OutputCount = self.Config["DNN.LayerNeurons"][-1]
    self.IsRegression = (self.OutputCount == 0)
    self.LayerNeurons = self.Config["DNN.LayerNeurons"][:-1]
    self.HiddenLayers = [None] * len(self.LayerNeurons)
    self.NormalizationLayers = [None] * len(self.LayerNeurons)
    self.DropOutLayer = None
    self.OutputLayer = None
    self.Classifier = None
    self.Input = None
    # ...................................................................
    if "DNN.ActivationFunction" not in self.Config:
      self.Config["DNN.ActivationFunction"] = "relu"

    if "DNN.DropoutRate" not in self.Config:
      self.IsDroppingOut = False
    else:
      self.IsDroppingOut = (self.Config["DNN.DropoutRate"] != 0.0)

    self.Create()
  # --------------------------------------------------------------------------------------
  @classmethod
  def from_config(cls, config):
    return cls(**config)
  # --------------------------------------------------------------------------------------
  def get_config(self):
    #config = super().get_config().copy()
    config = {"config": self.Config}
    return config
  # --------------------------------------------------------------------------------------
  def Create(self):
    # [FIX] In Tensorflow 2.17 we must assign the new object to a field for tthe summary() to work correctly
    for nIndex, nLayerNeuronCount in enumerate(self.LayerNeurons):
      self._layer = Dense(nLayerNeuronCount, activation=self.Config["DNN.ActivationFunction"], use_bias=True)
      self.HiddenLayers[nIndex] = self._layer

      self._layer = BatchNormalization()
      self.NormalizationLayers[nIndex] = self._layer

    if self.IsDroppingOut:
      self.DropOutLayer = Dropout(self.Config["DNN.DropoutRate"])

    self.OutputLayer = Dense(self.OutputCount, use_bias=not self.IsRegression)

    if not self.IsRegression:
      if self.OutputCount == 1:
        # Binary classification with threshold
        self.Classifier = Activation("sigmoid")
      else:
        # Using the Softmax activation function for the per-class neurons of the output layer
        self.Classifier = Softmax()
  # --------------------------------------------------------------------------------------
  def call(self, p_tInput):
    self.Input = p_tInput

    # Feed forward to the next layer
    tA = p_tInput
    for nIndex, oHiddenLayer in enumerate(self.HiddenLayers):
      oNormalizationLayer = self.NormalizationLayers[nIndex]
      tA = oHiddenLayer(tA)
      tA = oNormalizationLayer(tA)

    # Dropout before the output layer
    if self.IsDroppingOut:
      tA = self.DropOutLayer(tA)

    tA = self.OutputLayer(tA)

    if not self.IsRegression:
      tA = self.Classifier(tA)

    return tA
  # --------------------------------------------------------------------------------------

"""Custom loss function"""

# __________ // Create the Machine Learning model and training algorithm objects \\ __________
oDNN = CDNN(oConfig)
nInitialLearningRate = oConfig["Training.LearningRate"]

oCostFunction   = keras.losses.BinaryCrossentropy(from_logits=False)  #tf.keras.losses.CategoricalCrossentropy(from_logits=False)
oOptimizer      = keras.optimizers.SGD(learning_rate=nInitialLearningRate)

"""Shuffling"""

# Training data feed pipeline
nBatchSize = oConfig["Training.BatchSize"]

oTSDataFeed = tf.data.Dataset.from_tensor_slices((X_train, y_train))
oTSDataFeed = oTSDataFeed.shuffle(len(X_train))
oTSDataFeed = oTSDataFeed.batch(nBatchSize)
print("Training data object:", oTSDataFeed)

# Validation data feed pipeline
oVSDataFeed = tf.data.Dataset.from_tensor_slices((X_val, y_val))
oVSDataFeed = oVSDataFeed.batch(len(X_val))
print("Validation data object:", oVSDataFeed)

"""Fitting"""

oModelFS = oFileSys.models.subfs(oConfig.experiment_code)
sModelFolder = oModelFS.base_folder
sProcessLogFileName = "train.history"

if (not os.path.isdir(sModelFolder)) or IS_RETRAINING:
  oDNN.compile(loss=oCostFunction, optimizer=oOptimizer, metrics=["accuracy"])
  oProcessLog = oDNN.fit(  oTSDataFeed, batch_size=oConfig["Training.BatchSize"]
                            ,epochs=oConfig["Training.MaxEpoch"]
                            ,validation_data=oVSDataFeed
                          )
  print(f"Saving to {sModelFolder}")
  oDNN.export(sModelFolder)
  oHistory = oProcessLog.history
  oModelFS.obj.save(oHistory, sProcessLogFileName, is_overwriting=True)
else:
  print(f"Loading from {sModelFolder}")
  oDNN = keras.models.load_model(sModelFolder)
  oHistory = oModelFS.obj.load(sProcessLogFileName)
oDNN.summary()

"""Predicting"""

nClassificationThreshold = 0.5
nTargetClassLabels     = y_val

for batch_features, batch_labels in oVSDataFeed:
  nPredictedProbabilities = oDNN.predict(batch_features)
  nPredictedClassLabels  = np.where(nPredictedProbabilities>=nClassificationThreshold, 1, 0)

nPredictedClassLabels = nPredictedClassLabels.reshape(-1)

for i in range(len(batch_labels)):
        print("#%.2d Predicted:%d (Probabilities:%s) Actual:%d" % (i + 1, nPredictedClassLabels[i], nPredictedProbabilities[i], batch_labels[i].numpy()))
        print("  |__ Sum of all output neuron activations:%.3f" % np.sum(nPredictedProbabilities[i]))

"""Model Evaluation"""

from radnn.evaluation import EvaluateClassification

# We create an evaluator object that will produce several metrics
oEvaluator = EvaluateClassification(nTargetClassLabels, nPredictedClassLabels)
oEvaluator.print_per_class()
oEvaluator.print_overall()
oEvaluator.print_confusion_matrix()

"""Plotting"""

from radnn.plots import PlotConfusionMatrix

sPlotFileName = oFileSys.models.file(f'{oConfig.experiment_code}-ConfusionMatrix.png')
PlotConfusionMatrix(oEvaluator.confusion_matrix, f'{oConfig.experiment_code} Confusion Matrix').prepare().save(sPlotFileName).show()

from radnn.plots import PlotLearningCurve

sPlotFileName = oFileSys.models.file(f'{oConfig.experiment_code}-LearningCurve-%s.png')
oPlot = PlotLearningCurve(oHistory, oConfig.experiment_code)
oPlot.prepare().save(sPlotFileName % "Accuracy").show()
oPlot.prepare_cost("CCE").save(sPlotFileName % "CCE").show()